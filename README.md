# Machine Learning Model Security

## Description

This project aims to assess the security of machine learning models and includes various attack and defense methods. The project presents examples of attacks on models, such as attacks using fake data (FGSM Attack), model poisoning (Data Poisoning), as well as defense methods like adversarial training.

## Project Structure

- `config.yaml`: Configuration file for storing hyperparameters and other settings.
- `mlcpy.py`: Main script for data loading, model training, conducting attacks, and evaluating results.
- `requirements.txt`: List of project dependencies.
- `README.md`: Project description, structure, and installation instructions.

## Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/ml-security.git
   cd ml-security
